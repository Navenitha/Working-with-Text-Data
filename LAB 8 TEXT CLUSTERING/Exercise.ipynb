{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34edbe47-4190-419c-857c-0db6676fd148",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from tabulate import tabulate\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02e3f816-1748-41d2-8878-8abf4f4fd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function: Lowercasing, remove stopwords and stemming\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        preprocessed_docs = []\n",
    "        for doc in X:\n",
    "            # Lowercase the text\n",
    "            doc = doc.lower()\n",
    "            # Remove stopwords and apply stemming\n",
    "            doc = \" \".join([self.stemmer.stem(word) for word in doc.split() if word not in self.stop_words])\n",
    "            preprocessed_docs.append(doc)\n",
    "        return np.array(preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88b67bb7-1667-42f0-be49-42b66d0d39ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document                                           Predicted Cluster\n",
      "-----------------------------------------------  -------------------\n",
      "I love playing football on the weekends                            0\n",
      "I enjoy hiking and camping in the mountains                        0\n",
      "I like to read books and watch movies                              1\n",
      "I prefer playing video games over sports                           0\n",
      "I love listening to music and going to concerts                    0\n",
      "Purity: 0.8\n"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessed_data = preprocessor.fit_transform(dataset)\n",
    "\n",
    "# Step 4: Vectorization with TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "# Step 5: Perform clustering using KMeans\n",
    "k = 2 # Define number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the documents and their predicted clusters\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24ac7a79-af51-4e59-bbf9-cffd605be793",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exercise 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce05bf2b-0dbc-49b3-876a-d5b9d6b2e7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function: Lowercasing, remove stopwords and stemming\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        preprocessed_docs = []\n",
    "        for doc in X:\n",
    "            # Lowercase the text\n",
    "            doc = doc.lower()\n",
    "            # Remove stopwords and apply stemming\n",
    "            doc = \" \".join([self.stemmer.stem(word) for word in doc.split() if word not in self.stop_words])\n",
    "            preprocessed_docs.append(doc)\n",
    "        return np.array(preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d38ec8c9-672f-4af8-9872-79e617af0dcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Preprocessing function: Lowercasing, remove stopwords and stemming\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mTextPreprocessor\u001b[39;00m(BaseEstimator, TransformerMixin):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(stopwords\u001b[38;5;241m.\u001b[39mwords(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "# Preprocessing function: Lowercasing, remove stopwords and stemming\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        preprocessed_docs = []\n",
    "        for doc in X:\n",
    "            # Lowercase the text\n",
    "            doc = doc.lower()\n",
    "            # Remove stopwords and apply stemming\n",
    "            doc = \" \".join([self.stemmer.stem(word) for word in doc.split() if word not in self.stop_words])\n",
    "            preprocessed_docs.append(doc)\n",
    "        return np.array(preprocessed_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "540af1b1-325f-43e0-ac8b-e7efcc7555d5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TextPreprocessor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      2\u001b[0m dataset \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love playing football on the weekends\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      3\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI enjoy hiking and camping in the mountains\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      4\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI like to read books and watch movies\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI prefer playing video games over sports\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      6\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mI love listening to music and going to concerts\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Preprocess the dataset\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m TextPreprocessor()\n\u001b[0;32m     10\u001b[0m preprocessed_data \u001b[38;5;241m=\u001b[39m preprocessor\u001b[38;5;241m.\u001b[39mfit_transform(dataset)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Step 4: Tokenize and Train Word2Vec model\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'TextPreprocessor' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the dataset\n",
    "dataset = [\"I love playing football on the weekends\",\n",
    "           \"I enjoy hiking and camping in the mountains\",\n",
    "           \"I like to read books and watch movies\",\n",
    "           \"I prefer playing video games over sports\",\n",
    "           \"I love listening to music and going to concerts\"]\n",
    "\n",
    "# Preprocess the dataset\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessed_data = preprocessor.fit_transform(dataset)\n",
    "\n",
    "# Step 4: Tokenize and Train Word2Vec model\n",
    "tokenized_dataset = [doc.split() for doc in preprocessed_data]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_dataset, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Step 5: Create document embeddings\n",
    "X = np.array([np.mean([word2vec_model.wv[word] for word in doc.split() if word in word2vec_model.wv], axis=0)\n",
    "              for doc in preprocessed_data])\n",
    "\n",
    "# Step 6: Perform clustering using KMeans\n",
    "k = 2 # Define number of clusters\n",
    "km = KMeans(n_clusters=k)\n",
    "km.fit(X)\n",
    "\n",
    "# Predict the clusters for each document\n",
    "y_pred = km.predict(X)\n",
    "\n",
    "# Display the documents and their predicted clusters\n",
    "table_data = [[\"Document\", \"Predicted Cluster\"]]\n",
    "table_data.extend([[doc, cluster] for doc, cluster in zip(dataset, y_pred)])\n",
    "print(tabulate(table_data, headers=\"firstrow\"))\n",
    "\n",
    "# Calculate purity\n",
    "total_samples = len(y_pred)\n",
    "cluster_label_counts = [Counter(y_pred)]\n",
    "purity = sum(max(cluster.values()) for cluster in cluster_label_counts) / total_samples\n",
    "print(\"Purity:\", purity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d9ce32-0424-4cee-8615-414b12b8ccc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
